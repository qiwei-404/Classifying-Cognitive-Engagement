{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template Text Classifier\n",
    "This is a text classifier I used for one of my previous projects. The dataset has been redacted due to data protection issues. Feel free to use the code as a template for your own dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import random\n",
    "import nltk\n",
    "import math\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit, cross_validate, GridSearchCV, cross_val_predict, KFold, train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, make_scorer, cohen_kappa_score, f1_score, confusion_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download packages for preprocessing\n",
    "nltk.download('stopwords')\n",
    "nltk.download('names')\n",
    "nltk.download(\"brown\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"universal_tagset\")\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "The goal of preprocessing is to clean up noisy text that might affect the performance of the text classifier. I did the following steps in preprocessing: converting words to lower case, removing punctuation and normalizing non-standard words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to preprocess data\n",
    "import string\n",
    "from spacy.lang.en import English\n",
    "from normalise import normalise\n",
    "\n",
    "# Create our list of punctuation marks\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Create our list of stopwords\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "parser = English()\n",
    "\n",
    "\n",
    "# Creating our tokenizer function\n",
    "def spacy_tokenizer(sentence):\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    mytokens = parser(sentence)\n",
    "\n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in nlp(sentence) ]\n",
    "\n",
    "#     remove punctuation\n",
    "    mytokens = [tok for tok in mytokens if (tok not in punctuations)]\n",
    "\n",
    "#     normalise\n",
    "    if '-4th' in mytokens:\n",
    "        mytokens[mytokens.index('-4th')] = 'to the power of negative four'\n",
    "    else:    \n",
    "        mytokens = normalise(mytokens, verbose=False)\n",
    "    \n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens\n",
    "\n",
    "# convert list of tokens into one string\n",
    "def join_tokens(tokens):\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load your own dataset, replace \"MY_DATA\" with your own data path. The code here loads data from csv to pandas dataframe, but you can load other types of data as long as you change the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from csv\n",
    "data_path = os.path.join(os.getcwd(), \"MY_DATA\")\n",
    "data = pd.read_csv(data_path)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean data\n",
    "\n",
    "tokenized = []\n",
    "\n",
    "for row in labeled.itertuples():\n",
    "    text = row.text_without_emoji\n",
    "    tokens = spacy_tokenizer(text)\n",
    "    tokens = join_tokens(tokens)\n",
    "    tokenized.append(tokens)\n",
    "\n",
    "# Replace 'features' with the name of the feature column in your own data\n",
    "data['features'] = tokenized\n",
    "data.dropna(subset=['features'], inplace=True)\n",
    "\n",
    "clean_path = os.path.join(os.getcwd(), \"CLEAN_DATA\")\n",
    "labeled.to_csv(path_or_buf=clean_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you preprocess your text and save it in a csv, you can load the cleaned text directly next time you classify your text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load preprocessed data from csv\n",
    "data_path = os.path.join(os.getcwd(), \"CLEAN_DATA\")\n",
    "data = pd.read_csv(data_path)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier\n",
    "For baseline model, I build a pipeline to extract word features as TF-IDF vectors and classify text using random forest. I tune the model hyperparameters using grid search cross validation on stratified 10-fold data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define X, y\n",
    "# You may replace 'features' and 'labels' with your own features and labels column names\n",
    "X = data['features'].values\n",
    "y = data['labels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create text classification pipeline\n",
    "text_clf = Pipeline([('tfidf', TfidfVectorizer(analyzer='word', \n",
    "                                               ngram_range=(1, 2),\n",
    "                                               sublinear_tf=True)), \n",
    "                     ('clf', RandomForestClassifier(n_estimators=200, \n",
    "                                max_samples=None, criterion='gini',\n",
    "                                random_state=0, max_features=None, \n",
    "                                bootstrap=True, class_weight='balanced_subsample', \n",
    "                                n_jobs=-1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune pipeline parameters on stratified 10-fold\n",
    "skf = StratifiedKFold(n_splits=10, random_state=0, shuffle=True)\n",
    "parameters = {'clf__min_samples_leaf':[0.015, 0.02, 0.03, 0.04]}\n",
    "\n",
    "\n",
    "cohen_kappa = make_scorer(cohen_kappa_score)\n",
    "f1 = make_scorer(f1_score, average='macro')\n",
    "accuracy = make_scorer(accuracy_score)\n",
    "\n",
    "search = GridSearchCV(text_clf, parameters, cv=skf, scoring={'f1': f1, \n",
    "                        'accuracy': accuracy, 'cohen_kappa': cohen_kappa}, \n",
    "                      refit='f1', verbose=20, n_jobs=-1)\n",
    "\n",
    "\n",
    "\n",
    "search.fit(X, y)\n",
    "results = search.cv_results_\n",
    "print(results)\n",
    "print(search.best_params_)\n",
    "\n",
    "y_pred = cross_val_predict(search.best_estimator_, X, y, cv=skf, \n",
    "                           verbose=20, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Classifier\n",
    "I use the following code to visualize the classification process and performance. You may replace the class labels with the values in your own dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a randomly selected decision tree in the random forest\n",
    "\n",
    "# Extract single tree\n",
    "estimator = text_clf.named_steps.clf.estimators_[131]\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "\n",
    "dot_data = export_graphviz(estimator, out_file=None, \n",
    "                feature_names=text_clf.named_steps.tfidf.get_feature_names(), \n",
    "                          class_names=['1', '2', '3'], filled=True)\n",
    "\n",
    "graph = graphviz.Source(dot_data) \n",
    "graph.render(\"tfidf\")\n",
    "graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to plot confusion matrix\n",
    "def plot_confusion_matrix(cm, classes=None, title='Confusion matrix'):\n",
    "    \"\"\"Plots a confusion matrix.\"\"\"\n",
    "    if classes is not None:\n",
    "        sns.heatmap(cm, xticklabels=classes, yticklabels=classes, vmin=0., vmax=1., annot=True)\n",
    "    else:\n",
    "        sns.heatmap(cm, vmin=0., vmax=1.)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot confusion matrix\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "cm_norm = cm/cm.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm_norm, classes=['1', '2', '3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Classifier\n",
    "Now I use the Keras neural model in Tensorflow to classify my text. I compare the performance of this neural network with the random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data\n",
    "Since the Keras model in Tensorflow requires a different data input format, I need to convert our data to the format accepted by Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create a dataframe with features labels ONLY. Save it in a csv.\n",
    "text = data['features'].values\n",
    "labels = data['labels'].values\n",
    "xy_dict = {'features': text, 'labels': labels}\n",
    "xy_df = pd.DataFrame.from_dict(xy_dict)\n",
    "out_path = os.path.join(os.getcwd(), \"xy.csv\")\n",
    "xy_df.to_csv(path_or_buf=out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You only need to do the conversion once. You can load the dataset directly in the future.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper to create train, validation, test dataframes from csv\n",
    "def split_df_from_csv(data_path, feature='comments', target='CE_label', test_size=0.3, n_val_splits=10):\n",
    "    X_train_val, X_test, y_train_val, y_test = load_and_test_split(data_path, feature, target, test_size)\n",
    "    X_train, X_val, y_train, y_val = val_split(X_train_val, y_train_val, n_val_splits)\n",
    "    return splits_to_df(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "\n",
    "# helper to load data from csv and separate test split\n",
    "def load_and_test_split(data_path, feature, target, test_size):\n",
    "    df = pd.read_csv(data_path)\n",
    "    X = df[feature].values\n",
    "    y = df[target].values\n",
    "    \n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=test_size, random_state=0, \n",
    "                                                stratify=y)\n",
    "    return X_train_val, X_test, y_train_val, y_test\n",
    "\n",
    "# helper to create validation split\n",
    "def val_split(X, y, n_splits):     \n",
    "    sss = StratifiedShuffleSplit(n_splits=n_splits, random_state=0)\n",
    "    X_train, X_val, y_train, y_val = [], [], [], []\n",
    "    for train_index, val_index in sss.split(X, y):\n",
    "        X_train.append(X[train_index])\n",
    "        X_val.append(X[val_index])\n",
    "        y_train.append(y[train_index])\n",
    "        y_val.append(y[val_index])\n",
    "    return X_train, X_val, y_train, y_val\n",
    "\n",
    "# helper to create n dataframes from n splits\n",
    "def n_splits_to_df(X_splits, y_splits):\n",
    "    split_dfs = []\n",
    "    for i in range(len(X_splits)):\n",
    "        split_dict = {'comments': X_splits[i], 'CE_label': y_splits[i]}\n",
    "        split_df = pd.DataFrame.from_dict(split_dict)\n",
    "        split_dfs.append(split_df)\n",
    "    return split_dfs\n",
    "    \n",
    "#   helper to put split data in pandas dataframe\n",
    "def splits_to_df(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    test_dict = {'comments': X_test, 'CE_label': y_test}\n",
    "    test_df = pd.DataFrame.from_dict(test_dict)\n",
    "    train_dfs = n_splits_to_df(X_train, y_train)\n",
    "    val_dfs = n_splits_to_df(X_val, y_val)\n",
    "    return train_dfs, val_dfs, test_df\n",
    "\n",
    "# load reformatted data from csv into pandas dataframe\n",
    "data_path = os.path.join(os.getcwd(), \"comments_labels.csv\")\n",
    "train_dfs, val_dfs, test_df = split_df_from_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper to encode class labels as numeric values\n",
    "def encode_labels(labels):\n",
    "    labels_lookup = {'1': 0, '2': 1, '3': 2, '4': 3}\n",
    "    labels_encoded = np.vectorize(labels_lookup.get(labels))\n",
    "    return labels_encoded\n",
    "    \n",
    "# helper to convert a list of pandas dataframes to a list of tensorflow datasets\n",
    "def dfs_to_tf_data(dfs, example, target):\n",
    "    tf_sets = []\n",
    "    for df in dfs:\n",
    "        tf_set = tf.data.Dataset.from_tensor_slices((df[example].values, encode_labels(df[target].values)))\n",
    "        tf_sets.append(tf_set)\n",
    "    return tf_sets\n",
    "\n",
    "# helper to convert train, validation, test split dataframes to tensorflow dataset\n",
    "def splits_to_tf_data(train_dfs, val_dfs, test_df, example='features', target='labels'):\n",
    "    train_sets = dfs_to_tf_data(train_dfs, example, target)\n",
    "    val_sets = dfs_to_tf_data(val_dfs, example, target)\n",
    "    test_set = tf.data.Dataset.from_tensor_slices((test_df[example].values, encode_labelsde_labels(test_df[target].values)))\n",
    "    return train_sets, val_sets, test_set\n",
    "\n",
    "# convert train, validation, test dataframes into tensorflow datasets\n",
    "train_sets, val_sets, test_set = splits_to_tf_data(train_dfs, val_dfs, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Keras\n",
    "Now the data is formatted for Tensorflow, I can train my model using the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed training examples using model trained on google news\n",
    "embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
    "hub_layer = hub.KerasLayer(embedding, input_shape=[], \n",
    "                           dtype=tf.string, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the neural model\n",
    "model = tf.keras.Sequential()\n",
    "model.add(hub_layer)\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "train_set = train_sets[0]\n",
    "val_set = val_sets[0]\n",
    "history = model.fit(train_set.shuffle(309).batch(32), \n",
    "                    epochs=20,\n",
    "                    validation_data=val_set.batch(32),\n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "I test my model on the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting model results on test data.\n",
    "results = model.evaluate(test_data.batch(256), verbose=2)\n",
    "\n",
    "for name, value in zip(model.metrics_names, results):\n",
    "    print(\"%s: %.3f\" % (name, value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU-1.13",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
